import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import glob
from datetime import datetime, timedelta
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import traceback # Ensure traceback is imported
import json # For holdings display

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
from portfolios_optimization.data_loader import (
    load_price_data,
    load_return_data,
    load_model_actions,
    update_all_asset_data, # New
    create_combined_data,  # New
    generate_normalized_plot, # New
    generate_correlation_heatmap, # New
    generate_single_asset_plot # New
)
from portfolios_optimization.portfolio_optimizer import optimize_markowitz_portfolio
from portfolios_optimization.portfolio_analysis import calculate_metrics, plot_efficient_frontier
from portfolios_optimization.visualization import plot_portfolio_performance, plot_asset_allocation
from portfolios_optimization.model_trainer import train_model, load_trained_model
from portfolios_optimization.authentication import (
    initialize_users_file, register_user, authenticate_user, get_user_info,
    update_user_portfolio, get_user_portfolios, get_user_portfolio, get_portfolio_with_quantities,
    get_user_transactions
)
# <<< Import the new hypothetical simulator function >>>
from portfolios_optimization.hypothetical_simulator import run_hypothetical_analysis

# --- Imports needed for Recommendations --- #
from portfolio_analyzer import ( # Assuming these are defined in portfolio_analyzer.py
    preprocess_asset_data, # Need this helper
    FeatureEngineer,       # Need the class
    # StockPortfolioEnv,     # Might not be needed directly if DRLAgent handles env creation/reset
    DRLAgent,              # Need the agent class for prediction method
    INDICATORS,            # Need the list of indicators used for training
    STABLECOIN_ASSET,      # Need the stablecoin identifier
    softmax_normalization, # Need the normalization function
    # <<< REMOVE A2C, PPO, etc. from here >>>
)
# <<< Add direct import for SB3 models >>>
from stable_baselines3 import A2C, PPO, SAC, DDPG
# --- End Imports for Recommendations --- #

# <<< NEW: Import the analysis function >>>
from portfolio_analyzer import run_portfolio_analysis

# --- NEW: Imports for FinRobot/Autogen Chatbot ---
import autogen
# from finrobot.agents.workflow import SingleAssistant # Old import
from portfolios_optimization.finrobot_core.workflow import SingleAssistant # New import
# --- End Imports for FinRobot/Autogen Chatbot ---

# –ò–º–ø–æ—Ä—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
from app_pages import render_account_dashboard, render_transactions_manager

# --- HELPER FUNCTIONS (Moved to top after imports) ---

# --- Function to initialize FinRobot Agent ---
@st.cache_resource # Cache the agent resource itself
def initialize_finrobot_agent():
    """Initializes and returns a FinRobot agent configured with OAI_CONFIG_LIST."""
    try:
        llm_config = {
            "config_list": autogen.config_list_from_json(
                "OAI_CONFIG_LIST", # Assumes file is in the project root
                filter_dict={"model": ["gpt-4-0125-preview"]}, # Or whichever model you have in the list
            ),
            "timeout": 120,
            "temperature": 0.2, # Lower temperature for more factual answers
        }
        assistant_agent = SingleAssistant(
            name="Portfolio_Analyst_Assistant",
            system_message="You are a helpful AI assistant specialized in analyzing portfolio performance data. Answer the user's questions based on the provided portfolio summary. Be concise and clear.",
            llm_config=llm_config,
            human_input_mode="NEVER", 
        )
        return assistant_agent
    except FileNotFoundError:
        st.error("Error: OAI_CONFIG_LIST file not found. Please ensure it exists in the project root.")
        return None
    except Exception as e:
        st.error(f"Error initializing FinRobot agent: {e}")
        traceback.print_exc() 
        return None

# --- Function to format analysis results for LLM ---
def format_portfolio_data_for_llm(analysis_results):
    """Formats the portfolio analysis results into a string for the LLM agent."""
    if not analysis_results or not isinstance(analysis_results, dict):
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç."

    summary_parts = []

    # Extract metrics (raw numeric data)
    metrics_df = analysis_results.get('metrics') # Use the raw metrics
    if metrics_df is not None and isinstance(metrics_df, pd.DataFrame) and not metrics_df.empty:
        summary_parts.append("**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º:**")
        for strategy in metrics_df.index: # Iterate through strategies (index)
            summary_parts.append(f"\n*–°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}*" )
            for metric, value in metrics_df.loc[strategy].items(): # Iterate through metrics for the strategy
                if isinstance(value, (int, float)):
                    # Use original metric names from calculation for formatting clues
                    if any(p in metric.lower() for p in ['cagr', 'return']):
                        formatted_value = f"{value:.2%}"
                    elif any(p in metric.lower() for p in ['volatility', 'drawdown']):
                         formatted_value = f"{value:.2%}"
                    elif any(r in metric.lower() for r in ['ratio']):
                         formatted_value = f"{value:.2f}"
                    else: # Default for Final Value, Net Profit etc.
                         formatted_value = f"{value:,.2f}"
                         if 'value' in metric.lower() or 'profit' in metric.lower():
                              formatted_value = f"${formatted_value}" # Add dollar sign
                else:
                     formatted_value = str(value)
                summary_parts.append(f"  - {metric}: {formatted_value}")
        summary_parts.append("\n") 
    else:
        summary_parts.append("–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

    # Extract date range and final values from daily values DataFrame
    daily_values_df = analysis_results.get('portfolio_daily_values')
    if daily_values_df is not None and isinstance(daily_values_df, pd.DataFrame) and not daily_values_df.empty:
        start_date = daily_values_df.index.min().strftime('%Y-%m-%d')
        end_date = daily_values_df.index.max().strftime('%Y-%m-%d')
        summary_parts.append(f"**–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:** {start_date} - {end_date}\n")

        summary_parts.append("**–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º:**")
        final_values = daily_values_df.iloc[-1] # Get last row
        # Filter only strategy value columns (usually start with 'Value_')
        strategy_value_cols = [col for col in daily_values_df.columns if col.startswith('Value_')]
        for strategy_col in strategy_value_cols:
            # Try to map column name back to display name if possible (e.g., from metrics index)
            strategy_name = strategy_col.replace('Value_', '').replace('_', ' ') # Basic name cleanup
            if metrics_df is not None and not metrics_df.empty:
                 matching_names = [idx for idx in metrics_df.index if strategy_col.endswith(idx.replace(' ','_').replace('DRL ',''))]
                 if matching_names: strategy_name = matching_names[0]
            
            value = final_values.get(strategy_col, np.nan)
            if pd.notna(value):
                 summary_parts.append(f"  - {strategy_name}: ${value:,.2f}")
        summary_parts.append("\n")
    else:
        summary_parts.append("–î–∞–Ω–Ω—ã–µ –æ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

    return "\n".join(summary_parts)

# --- END HELPER FUNCTIONS ---

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Portfolio Optimization System",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# <<< Add the missing function definition here >>>
# Function to load combined data with caching
# Use a parameter that changes upon update to invalidate cache
@st.cache_data
def load_combined_data_cached(update_trigger):
    """Loads combined data from CSV, using update_trigger for cache invalidation."""
    print(f"Cache Trigger: {update_trigger}. Loading combined data...") # Debug print
    combined_data_path = os.path.join("data", "data_compare_eda.csv")
    if os.path.exists(combined_data_path):
        try:
            combined_df = pd.read_csv(combined_data_path, index_col=0, parse_dates=True)
            # Ensure index is DatetimeIndex
            if not isinstance(combined_df.index, pd.DatetimeIndex):
                 combined_df.index = pd.to_datetime(combined_df.index, errors='coerce')
                 combined_df = combined_df.dropna(axis=0, subset=[combined_df.index.name]) # Drop rows if date parse failed
            combined_df.sort_index(inplace=True)
            print(f"Loaded {combined_data_path}, shape: {combined_df.shape}")
            return combined_df
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {combined_data_path}: {e}")
            return pd.DataFrame()
    else:
        st.warning(f"–§–∞–π–ª {combined_data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return pd.DataFrame()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
initialize_users_file()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False
if 'username' not in st.session_state:
    st.session_state.username = None
if 'login_message' not in st.session_state:
    st.session_state.login_message = None
if 'active_page' not in st.session_state:
    st.session_state.active_page = "Login"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –∞–∫–∫–∞—É–Ω—Ç–∞
def logout():
    st.session_state.logged_in = False
    st.session_state.username = None
    st.session_state.active_page = "Login"
    st.session_state.login_message = "–í—ã –≤—ã—à–ª–∏ –∏–∑ —Å–∏—Å—Ç–µ–º—ã"
    # Clear analysis results on logout
    if 'analysis_results' in st.session_state: del st.session_state['analysis_results']
    if 'analysis_figure' in st.session_state: del st.session_state['analysis_figure']

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@st.cache_data(ttl=3600)
def load_data():
    # –î–∞–Ω–Ω—ã–µ —Ü–µ–Ω –∞–∫—Ç–∏–≤–æ–≤
    price_data = load_price_data()
    
    # –î–∞–Ω–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
    model_returns = load_return_data()
    
    # –î–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª–µ–π –º–æ–¥–µ–ª–µ–π
    model_actions = load_model_actions()
    
    return price_data, model_returns, model_actions

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
price_data, model_returns, model_actions = load_data()

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–∫—Ç–∏–≤–æ–≤
assets = price_data.columns.tolist() if not price_data.empty else []

# –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("Investment Portfolio Monitoring & Optimization System")

# –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
if not st.session_state.logged_in:
    # –í–∫–ª–∞–¥–∫–∏ –¥–ª—è –≤—Ö–æ–¥–∞ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    tab1, tab2 = st.tabs(["–í—Ö–æ–¥", "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"])
    
    # –í–∫–ª–∞–¥–∫–∞ –≤—Ö–æ–¥–∞
    with tab1:
        st.header("–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É")
        
        # –§–æ—Ä–º–∞ –≤—Ö–æ–¥–∞
        with st.form("login_form"):
            username = st.text_input("–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password")
            submit_button = st.form_submit_button("–í–æ–π—Ç–∏")
            
            if submit_button:
                success, message = authenticate_user(username, password)
                
                if success:
                    st.session_state.logged_in = True
                    st.session_state.username = username
                    st.session_state.active_page = "–ú–æ–π –∫–∞–±–∏–Ω–µ—Ç"
                    st.success(message)
                    st.rerun()
                else:
                    st.error(message)
        
        # –°–æ–æ–±—â–µ–Ω–∏–µ –æ –≤—ã—Ö–æ–¥–µ –∏–ª–∏ –æ—à–∏–±–∫–µ
        if st.session_state.login_message:
            st.info(st.session_state.login_message)
            st.session_state.login_message = None
    
    # –í–∫–ª–∞–¥–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    with tab2:
        st.header("–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
        
        # –§–æ—Ä–º–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
        with st.form("register_form"):
            new_username = st.text_input("–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            new_password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password")
            confirm_password = st.text_input("–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–∞—Ä–æ–ª—è", type="password")
            email = st.text_input("Email")
            register_button = st.form_submit_button("–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è")
            
            if register_button:
                if not new_username or not new_password:
                    st.error("–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø–∞—Ä–æ–ª—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã")
                elif new_password != confirm_password:
                    st.error("–ü–∞—Ä–æ–ª–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç")
                else:
                    success, message = register_user(new_username, new_password, email)
                    
                    if success:
                        st.success(message)
                        st.info("–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –≤–æ–π—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º—É")
                    else:
                        st.error(message)

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
else:
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    st.sidebar.header(f"–ü—Ä–∏–≤–µ—Ç, {st.session_state.username}!")
    
    # –ö–Ω–æ–ø–∫–∞ –≤—ã—Ö–æ–¥–∞ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
    if st.sidebar.button("–í—ã–π—Ç–∏"):
        logout()
        st.rerun()
    
    # –ú–µ–Ω—é –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    st.sidebar.header("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
    page_options = [
        "–ú–æ–π –∫–∞–±–∏–Ω–µ—Ç", 
        "–î–∞–Ω–Ω—ã–µ –∏ –ê–Ω–∞–ª–∏–∑", 
        "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", # Added "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"
        "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏", 
        "–ï–¥–∏–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç", 
        "–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è", 
        "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
    ]
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è radio –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–µ—Å—Å–∏–∏
    try:
        if st.session_state.active_page not in page_options:
            st.session_state.active_page = page_options[0]
        current_page_index = page_options.index(st.session_state.active_page)
    except ValueError:
        current_page_index = 0
        st.session_state.active_page = page_options[0]

    selected_page = st.sidebar.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª",
        page_options,
        index=current_page_index,
        key="main_nav_radio"
    )
    
    if selected_page != st.session_state.active_page:
        st.session_state.active_page = selected_page
        st.rerun()

    # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ª–∏—á–Ω–æ–≥–æ –∫–∞–±–∏–Ω–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if st.session_state.active_page == "–ú–æ–π –∫–∞–±–∏–Ω–µ—Ç":
        st.header("–õ–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
        user_info = get_user_info(st.session_state.username)
        
        if user_info:
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.subheader("–í–∞—à –ø—Ä–æ—Ñ–∏–ª—å")
                st.write(f"**Email:** {user_info.get('email', '–ù–µ —É–∫–∞–∑–∞–Ω')}")
                st.write(f"**–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏:** {user_info.get('created_at', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                st.write(f"**–ü–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ö–æ–¥:** {user_info.get('last_login', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            st.subheader("–í–∞—à–∏ –ø–æ—Ä—Ç—Ñ–µ–ª–∏")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω–æ–º –ø–æ—Ä—Ç—Ñ–µ–ª–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            portfolio_data = get_portfolio_with_quantities(st.session_state.username)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∞–∫—Ç–∏–≤–æ–≤ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ
            has_assets = portfolio_data and any(portfolio_data["quantities"].values())
            
            if has_assets:
                # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
                portfolio_items = []
                
                total_portfolio_value = 0
                total_profit_loss = 0
                
                for asset, quantity in portfolio_data["quantities"].items():
                    if quantity > 0:
                        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã –∞–∫—Ç–∏–≤–∞
                        current_price = price_data[asset].iloc[-1] if asset in price_data.columns else 0
                        
                        # –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –ø–æ–∫—É–ø–∫–∏
                        avg_buy_price = portfolio_data["avg_prices"].get(asset, 0)
                        
                        # –†–∞—Å—á–µ—Ç —Ç–µ–∫—É—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–∞
                        current_value = quantity * current_price
                        invested_value = quantity * avg_buy_price
                        profit_loss = current_value - invested_value
                        profit_loss_percent = (profit_loss / invested_value * 100) if invested_value > 0 else 0
                        
                        total_portfolio_value += current_value
                        total_profit_loss += profit_loss
                        
                        portfolio_items.append({
                            "–ê–∫—Ç–∏–≤": asset,
                            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ": quantity,
                            "–°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –ø–æ–∫—É–ø–∫–∏": avg_buy_price,
                            "–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞": current_price,
                            "–¢–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å": current_value,
                            "P&L": profit_loss,
                            "P&L (%)": profit_loss_percent
                        })
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Ä—Ç—Ñ–µ–ª–µ
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è", f"${total_portfolio_value:,.2f}")
                
                with col2:
                    # –ó–Ω–∞–∫ –¥–ª—è P&L
                    if total_profit_loss >= 0:
                        st.metric("–û–±—â–∏–π P&L", f"${total_profit_loss:,.2f}", delta=f"{total_profit_loss/total_portfolio_value*100:.2f}%")
                    else:
                        st.metric("–û–±—â–∏–π P&L", f"-${abs(total_profit_loss):,.2f}", delta=f"{total_profit_loss/total_portfolio_value*100:.2f}%", delta_color="inverse")
                
                with col3:
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∑–∞ 24 —á–∞—Å–∞
                    portfolio_24h_ago = 0
                    for item in portfolio_items:
                        asset = item["–ê–∫—Ç–∏–≤"]
                        quantity = item["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"]
                        price_24h_ago = price_data[asset].iloc[-2] if asset in price_data.columns and len(price_data) > 1 else item["–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞"]
                        portfolio_24h_ago += quantity * price_24h_ago
                    
                    change_24h = (total_portfolio_value - portfolio_24h_ago) / portfolio_24h_ago * 100 if portfolio_24h_ago > 0 else 0
                    
                    st.metric("–ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ 24—á", 
                             f"${total_portfolio_value - portfolio_24h_ago:,.2f}", 
                             delta=f"{change_24h:.2f}%",
                             delta_color="normal" if change_24h >= 0 else "inverse")
                
                # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Ä—Ç—Ñ–µ–ª—è
                portfolio_df = pd.DataFrame(portfolio_items)
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∞–∫—Ç–∏–≤–æ–≤
                if not portfolio_df.empty:
                    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–∫—É—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
                    portfolio_df = portfolio_df.sort_values("–¢–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å", ascending=False)
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    formatted_df = portfolio_df.copy()
                    formatted_df["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"] = formatted_df["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"].apply(lambda x: f"{x:,.8f}")
                    formatted_df["–°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –ø–æ–∫—É–ø–∫–∏"] = formatted_df["–°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –ø–æ–∫—É–ø–∫–∏"].apply(lambda x: f"${x:,.2f}")
                    formatted_df["–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞"] = formatted_df["–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞"].apply(lambda x: f"${x:,.2f}")
                    formatted_df["–¢–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å"] = formatted_df["–¢–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å"].apply(lambda x: f"${x:,.2f}")
                    formatted_df["P&L"] = formatted_df["P&L"].apply(
                        lambda x: f"${x:,.2f}" if x >= 0 else f"-${abs(x):,.2f}"
                    )
                    formatted_df["P&L (%)"] = formatted_df["P&L (%)"].apply(
                        lambda x: f"+{x:.2f}%" if x > 0 else (f"{x:.2f}%" if x < 0 else "0.00%")
                    )
                    
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                    st.dataframe(formatted_df, use_container_width=True)
                    
                    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–æ–≤ –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
                    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è")
                    fig = px.pie(
                        portfolio_df,
                        values="–¢–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å",
                        names="–ê–∫—Ç–∏–≤",
                        title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è –ø–æ —Ç–µ–∫—É—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    st.info("–ß—Ç–æ–±—ã –∏–∑–º–µ–Ω–∏—Ç—å —Å–æ—Å—Ç–∞–≤ –ø–æ—Ä—Ç—Ñ–µ–ª—è, –¥–æ–±–∞–≤—å—Ç–µ –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –∞–∫—Ç–∏–≤—ã —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏'.")
            else:
                st.info("""
                –£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç –∞–∫—Ç–∏–≤–æ–≤ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ.
                
                –ß—Ç–æ–±—ã —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—å:
                1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏'
                2. –ù–∞ –≤–∫–ª–∞–¥–∫–µ '–î–æ–±–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é' –¥–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –ø–µ—Ä–≤—ã–µ –∞–∫—Ç–∏–≤—ã
                3. –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, –ø–æ—Ä—Ç—Ñ–µ–ª—å —Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                
                –í–∞—à –ø–æ—Ä—Ç—Ñ–µ–ª—å –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –∑–¥–µ—Å—å –∏ –≤ —Ä–∞–∑–¥–µ–ª–µ '–ï–¥–∏–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç'.
                """)
                
                # –ö–Ω–æ–ø–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ —Ä–∞–∑–¥–µ–ª—É "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏"
                if st.button("–ü–µ—Ä–µ–π—Ç–∏ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∞–∫—Ç–∏–≤–∞–º–∏", key="goto_manage_assets_from_cabinet"):
                    st.session_state.active_page = "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏"
                    st.rerun()
        else:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ")
    
    # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –µ–¥–∏–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –≤ —Å—Ç–∏–ª–µ Bybit
    elif st.session_state.active_page == "–ï–¥–∏–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç":
        st.header("–ï–¥–∏–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç")
        st.markdown("--- ")

        # --- Imports for this page ---
        from collections import OrderedDict
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import plotly.io as pio
        import plotly.express as px

        # --- Load User Transactions --- 
        username = st.session_state.username
        transactions_list = get_user_transactions(username)

        # Convert list of transactions to DataFrame
        if transactions_list:
             transactions_df_raw = pd.DataFrame(transactions_list)
             # Ensure correct dtypes after loading from JSON
             transactions_df_raw['date'] = pd.to_datetime(transactions_df_raw['date'])
             transactions_df_raw['quantity'] = pd.to_numeric(transactions_df_raw['quantity'])
             transactions_df_raw['price'] = pd.to_numeric(transactions_df_raw['price'])
             transactions_df_raw['fee'] = pd.to_numeric(transactions_df_raw.get('fee', 0))
             if 'total_cost' not in transactions_df_raw.columns:
                  transactions_df_raw['total_cost'] = transactions_df_raw['quantity'] * transactions_df_raw['price'] + transactions_df_raw['fee']
             else:
                   transactions_df_raw['total_cost'] = pd.to_numeric(transactions_df_raw['total_cost'])
             # Sort transactions chronologically - IMPORTANT
             transactions_df_raw = transactions_df_raw.sort_values(by='date').reset_index(drop=True)
        else:
            transactions_df_raw = pd.DataFrame()

        if transactions_df_raw.empty:
            st.info("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π. –î–æ–±–∞–≤—å—Ç–µ –∏—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏'.")
            if st.button("–ü–µ—Ä–µ–π—Ç–∏ –∫ –£–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∞–∫—Ç–∏–≤–∞–º–∏", key="uta_goto_manage"):
                 st.session_state.active_page = "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏"
                 st.rerun()
            st.stop()

        # Filter only buy transactions for the core logic of the notebook
        # Note: Sell transactions are ignored in the notebook's P&L logic, only used for markers
        buy_transactions_df = transactions_df_raw[transactions_df_raw['type'] == 'buy'].copy()
        # We need an ID for each buy transaction for the logic
        buy_transactions_df['Purchase_ID'] = buy_transactions_df.index # Simple ID based on order

        if buy_transactions_df.empty:
            st.info("–í –∏—Å—Ç–æ—Ä–∏–∏ –Ω–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ–∫—É–ø–∫–∏. –ì—Ä–∞—Ñ–∏–∫ –¥–∏–Ω–∞–º–∏–∫–∏ P&L –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω.")
            # We can still show current holdings based on all transactions
            # (Add logic here later if needed to show holdings even without buys)
            st.stop()

        # --- Calculate Current Holdings (based on ALL transactions) --- 
        holdings = {}
        for _, row in transactions_df_raw.iterrows():
            asset = row['asset']
            quantity = row['quantity']
            type = row['type']
            if asset not in holdings: holdings[asset] = 0
            if type == 'buy': holdings[asset] += quantity
            elif type == 'sell': holdings[asset] -= quantity
        current_holdings = {asset: q for asset, q in holdings.items() if q > 1e-9} # Tolerance
        required_assets = list(current_holdings.keys())

        if not required_assets:
            st.info("–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —É –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–æ–≤ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ.")
            st.stop()

        # --- Load Historical Data (Function definition kept separate for clarity) --- 
        @st.cache_data(ttl=1800)
        def load_and_preprocess_historical_data_uta(assets_list):
            csv_base_path = 'D:\\__projects__\\diploma\\portfolios-optimization\\data' # Consider making this relative or configurable
            all_prices = {}
            data_found = False
            min_start_date = pd.Timestamp.max.tz_localize(None)
            for asset in assets_list:
                file_path = os.path.join(csv_base_path, f'{asset}_hourly_data.csv')
                try:
                    # --- Robust Date Parsing --- 
                    # Read Open time as string first
                    df = pd.read_csv(file_path, dtype={'Open time': str}) 
                    
                    # Rename columns if necessary (case-insensitive search)
                    time_col_found = 'Open time' # Assume default first
                    if 'Open time' not in df.columns:
                        time_col_found = next((c for c in df.columns if 'time' in c.lower()), None)
                        if time_col_found: df.rename(columns={time_col_found:'Open time'}, inplace=True)
                        else: raise ValueError("Timestamp column ('Open time' or similar) not found.")

                    price_col_found = 'Close' # Assume default first
                    if 'Close' not in df.columns:
                        price_col_found = next((c for c in df.columns if 'close' in c.lower()), None)
                        if price_col_found: df.rename(columns={price_col_found:'Close'}, inplace=True)
                        else: raise ValueError("Price column ('Close' or similar) not found.")
                    
                    # Parse dates robustly
                    parsed_ms = pd.to_datetime(df['Open time'], unit='ms', errors='coerce')
                    parsed_str = pd.to_datetime(df.loc[parsed_ms.isna(), 'Open time'], errors='coerce')
                    df['date_index'] = parsed_ms.fillna(parsed_str)
                    df.dropna(subset=['date_index'], inplace=True)
                    df = df.set_index('date_index')
                    # --- End Robust Date Parsing ---

                    df = df[['Close']].rename(columns={'Close': f'{asset}_Price'})
                    df[f'{asset}_Price'] = pd.to_numeric(df[f'{asset}_Price'], errors='coerce') # Ensure numeric
                    df.dropna(subset=[f'{asset}_Price'], inplace=True) # Drop if Close price is invalid
                    
                    if not df.empty:
                        all_prices[asset] = df
                        data_found = True
                        min_start_date = min(min_start_date, df.index.min())
                        
                except FileNotFoundError:
                     st.warning(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {asset}: {file_path}")
                except Exception as e:
                    st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏/–æ–±—Ä–∞–±–æ—Ç–∫–∏ {asset}: {e}")
            if not data_found: return pd.DataFrame(), pd.Timestamp.now().tz_localize(None)
            if min_start_date == pd.Timestamp.max.tz_localize(None): min_start_date = pd.Timestamp.now().tz_localize(None) - pd.Timedelta(days=1)
            return pd.concat(all_prices.values(), axis=1), min_start_date

        historical_prices, earliest_data_date = load_and_preprocess_historical_data_uta(required_assets)

        if historical_prices.empty:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–∫—Ç–∏–≤–æ–≤ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ.")
            st.stop()

        # --- Calculate Metrics (Current State) --- 
        latest_prices = historical_prices.ffill().iloc[-1]
        prices_24h_ago = historical_prices.ffill().iloc[-25] if len(historical_prices) >= 25 else latest_prices
        
        total_balance = 0
        total_balance_24h_ago = 0
        current_holdings_list = []
        total_cost_basis_from_all_tx = 0 # Recalculate cost basis from raw transactions for accuracy
        temp_holdings_for_cost = {}
        for _, row in transactions_df_raw.iterrows():
            asset = row['asset']
            q = row['quantity']
            cost = row['total_cost']
            type = row['type']
            if asset not in temp_holdings_for_cost: temp_holdings_for_cost[asset] = {'q':0, 'cost':0}
            if type == 'buy': 
                temp_holdings_for_cost[asset]['q'] += q
                temp_holdings_for_cost[asset]['cost'] += cost
            elif type == 'sell':
                if temp_holdings_for_cost[asset]['q'] > 1e-9: # Check if holding exists
                    ratio = min(q / temp_holdings_for_cost[asset]['q'], 1.0)
                    temp_holdings_for_cost[asset]['cost'] *= (1 - ratio)
                    temp_holdings_for_cost[asset]['q'] -= q
                temp_holdings_for_cost[asset]['q'] = max(0, temp_holdings_for_cost[asset]['q'])
        total_cost_basis_from_all_tx = sum(d['cost'] for d in temp_holdings_for_cost.values() if d['q'] > 1e-9)

        for asset, quantity in current_holdings.items():
            current_price = latest_prices.get(f'{asset}_Price', 0)
            price_24h = prices_24h_ago.get(f'{asset}_Price', current_price)
            current_value = quantity * current_price
            value_24h = quantity * price_24h
            total_balance += current_value
            total_balance_24h_ago += value_24h
            current_holdings_list.append({"–ê–∫—Ç–∏–≤": asset, "–ö–æ–ª-–≤–æ": quantity, "–°—Ç–æ–∏–º–æ—Å—Ç—å (USD)": current_value})

        today_pnl_usd = total_balance - total_balance_24h_ago
        today_pnl_pct = (today_pnl_usd / total_balance_24h_ago * 100) if total_balance_24h_ago > 0 else 0
        total_pnl_usd = total_balance - total_cost_basis_from_all_tx
        total_pnl_pct = (total_pnl_usd / total_cost_basis_from_all_tx * 100) if total_cost_basis_from_all_tx > 0 else 0

        # --- Display Metrics --- 
        col1, col2, col3 = st.columns(3)
        with col1: st.metric("–ë–∞–ª–∞–Ω—Å –∞–∫–∫–∞—É–Ω—Ç–∞ (USD)", f"{total_balance:,.2f}")
        with col2: st.metric("P&L –∑–∞ —Å–µ–≥–æ–¥–Ω—è (USD)", f"{today_pnl_usd:,.2f}", delta=f"{today_pnl_pct:.2f}%", delta_color="normal" if today_pnl_usd >= 0 else "inverse")
        with col3: st.metric("–û–±—â–∏–π P&L –ø–æ—Ä—Ç—Ñ–µ–ª—è (USD)", f"{total_pnl_usd:,.2f}", delta=f"{total_pnl_pct:.2f}%", delta_color="normal" if total_pnl_usd >= 0 else "inverse")
        st.markdown("--- ")
        
        # --- Assets Overview --- 
        col_assets, col_chart_placeholder = st.columns([1, 2]) # Placeholder for chart area
        with col_assets:
            st.subheader("–ê–∫—Ç–∏–≤—ã")
            if current_holdings_list:
                holdings_df = pd.DataFrame(current_holdings_list)
                holdings_df["–î–æ–ª—è (%)"] = (holdings_df["–°—Ç–æ–∏–º–æ—Å—Ç—å (USD)"] / total_balance * 100).round(2) if total_balance > 0 else 0
                holdings_df = holdings_df.sort_values("–°—Ç–æ–∏–º–æ—Å—Ç—å (USD)", ascending=False)
                st.dataframe(holdings_df.style.format({"–ö–æ–ª-–≤–æ": "{:.6f}", "–°—Ç–æ–∏–º–æ—Å—Ç—å (USD)": "${:,.2f}", "–î–æ–ª—è (%)": "{:.2f}%"}), use_container_width=True)
            else: st.write("–ù–µ—Ç –∞–∫—Ç–∏–≤–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

        # --- Detailed Historical Calculation (from Notebook logic) ---
        st.markdown("--- ")
        st.subheader(f"–ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è")
        
        days_history_options = {"7 –¥–Ω–µ–π": 7, "30 –¥–Ω–µ–π": 30, "90 –¥–Ω–µ–π": 90, "180 –¥–Ω–µ–π": 180, "–í—Å–µ –≤—Ä–µ–º—è": None}
        selected_days_label = st.radio("–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:", days_history_options.keys(), index=3, horizontal=True, key="analysis_interval") # Default 180d
        selected_days = days_history_options[selected_days_label]
        
        report_date_viz = historical_prices.index.max()
        if selected_days:
            start_date_viz = report_date_viz - pd.Timedelta(days=selected_days)
        else:
            start_date_viz = transactions_df_raw['date'].min()
        start_date_viz = max(start_date_viz, earliest_data_date) # Ensure we don't go before data exists

        historical_prices_filtered = historical_prices[
            (historical_prices.index >= start_date_viz) &
            (historical_prices.index <= report_date_viz)
        ].copy()
        historical_prices_filtered = historical_prices_filtered.ffill().bfill().dropna(how='all')

        if historical_prices_filtered.empty:
            st.warning(f"–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø–µ—Ä–∏–æ–¥–µ ({selected_days_label}).")
        else:
            # Find actual purchase prices within the *full* historical data for accuracy
            buy_transactions_df['Purchase_Price_Actual'] = np.nan
            buy_transactions_df['Actual_Purchase_Time_Index'] = pd.NaT
            for index, row in buy_transactions_df.iterrows():
                asset = row['asset']
                purchase_date = row['date']
                price_col = f'{asset}_Price'
                if price_col not in historical_prices.columns: continue
                relevant_prices_index = historical_prices.index[historical_prices.index >= purchase_date]
                if not relevant_prices_index.empty:
                    actual_purchase_time_index = relevant_prices_index[0]
                    try:
                        purchase_price = historical_prices.loc[actual_purchase_time_index, price_col]
                        if pd.notna(purchase_price) and purchase_price > 0:
                            buy_transactions_df.loc[index, 'Purchase_Price_Actual'] = purchase_price
                            buy_transactions_df.loc[index, 'Actual_Purchase_Time_Index'] = actual_purchase_time_index
                    except KeyError: pass # Ignore if time index not found exactly
            
            # Drop buys where we couldn't find a valid price/time
            buy_transactions_df.dropna(subset=['Actual_Purchase_Time_Index', 'Purchase_Price_Actual'], inplace=True)

            if buy_transactions_df.empty:
                st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ–∫—É–ø–∫–∏ —Å —Ü–µ–Ω–∞–º–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ P&L.")
            else:
                # Calculate Cumulative Cost based *only* on the valid buys found
                historical_prices_filtered['Cumulative_Cost'] = 0.0
                for _, row in buy_transactions_df.iterrows():
                    cost = row['total_cost']
                    purchase_time = row['Actual_Purchase_Time_Index']
                    historical_prices_filtered.loc[historical_prices_filtered.index >= purchase_time, 'Cumulative_Cost'] += cost

                # Calculate individual value, P&L, contribution
                purchase_value_cols = []
                purchase_pnl_cols = []
                purchase_perc_contrib_cols = []
                purchase_labels = []
                
                with st.spinner("–†–∞—Å—á–µ—Ç –¥–∏–Ω–∞–º–∏–∫–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è..."):
                    for index, purchase_row in buy_transactions_df.iterrows():
                        purchase_id = purchase_row['Purchase_ID']
                        asset = purchase_row['asset']
                        initial_investment = purchase_row['total_cost']
                        purchase_price = purchase_row['Purchase_Price_Actual']
                        purchase_time = purchase_row['Actual_Purchase_Time_Index']
                        price_col = f'{asset}_Price'
                        
                        if price_col not in historical_prices_filtered.columns: continue

                        value_col_name = f"Value_ID{purchase_id}_{asset}"
                        pnl_col_name = f"PnL_ID{purchase_id}_{asset}"
                        perc_contrib_col_name = f"PercContrib_ID{purchase_id}_{asset}"
                        label = f"{asset} (ID:{purchase_id}, ${initial_investment:,.2f})"

                        purchase_value_cols.append(value_col_name)
                        purchase_pnl_cols.append(pnl_col_name)
                        purchase_perc_contrib_cols.append(perc_contrib_col_name)
                        purchase_labels.append(label)

                        historical_prices_filtered[value_col_name] = 0.0
                        historical_prices_filtered[pnl_col_name] = 0.0
                        historical_prices_filtered[perc_contrib_col_name] = 0.0

                        mask = historical_prices_filtered.index >= purchase_time
                        if mask.any():
                            current_prices = historical_prices_filtered.loc[mask, price_col]
                            if pd.isna(purchase_price) or purchase_price <= 0:
                                price_ratio = pd.Series(0.0, index=current_prices.index)
                            else:
                                price_ratio = current_prices / purchase_price
                                price_ratio = price_ratio.fillna(0).replace([np.inf, -np.inf], 0)
                            current_purchase_value = initial_investment * price_ratio
                            historical_prices_filtered.loc[mask, value_col_name] = current_purchase_value
                            historical_prices_filtered.loc[mask, pnl_col_name] = current_purchase_value - initial_investment

                    # Sum up totals
                    historical_prices_filtered['Total_Value_Relative'] = historical_prices_filtered[purchase_value_cols].sum(axis=1)
                    historical_prices_filtered['Total_PnL'] = historical_prices_filtered['Total_Value_Relative'] - historical_prices_filtered['Cumulative_Cost']
                    
                    # Calculate percentage contributions
                    denom = historical_prices_filtered['Total_Value_Relative']
                    valid_denom_mask = np.abs(denom) > 1e-9
                    for pnl_col, perc_contrib_col in zip(purchase_pnl_cols, purchase_perc_contrib_cols):
                        percentage_contribution = np.zeros_like(denom)
                        percentage_contribution[valid_denom_mask] = (historical_prices_filtered.loc[valid_denom_mask, pnl_col] / denom[valid_denom_mask]) * 100
                        historical_prices_filtered[perc_contrib_col] = pd.Series(percentage_contribution, index=historical_prices_filtered.index).fillna(0)
                    
                    total_pnl_percentage = np.zeros_like(denom)
                    total_pnl_percentage[valid_denom_mask] = (historical_prices_filtered.loc[valid_denom_mask, 'Total_PnL'] / denom[valid_denom_mask]) * 100
                    historical_prices_filtered['Total_PnL_Percentage'] = pd.Series(total_pnl_percentage, index=historical_prices_filtered.index).fillna(0)

                # --- Plotting --- 
                pio.templates.default = "plotly_dark"
                fig = make_subplots(
                    rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.04,
                    subplot_titles=(
                        f'–°—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è vs –í–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞',
                        '–í–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –ê–±—Å–æ–ª—é—Ç–Ω—ã–π P&L',
                        '–í–∫–ª–∞–¥ P&L –∫–∞–∂–¥–æ–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ % –æ—Ç –û–±—â–µ–π –°—Ç–æ–∏–º–æ—Å—Ç–∏'
                    ))

                # Chart 1
                fig.add_trace(go.Scatter(x=historical_prices_filtered.index, y=historical_prices_filtered['Total_Value_Relative'], mode='lines', name='–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å', line=dict(color='#388BFF', width=2), hovertemplate='–î–∞—Ç–∞: %{x}<br>–°—Ç–æ–∏–º–æ—Å—Ç—å: %{y:,.2f} USDT<extra></extra>'), row=1, col=1)
                fig.add_trace(go.Scatter(x=historical_prices_filtered.index, y=historical_prices_filtered['Cumulative_Cost'], mode='lines', name='–í–ª–æ–∂–µ–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤', line=dict(color='#AAAAAA', dash='dash', width=1.5), hovertemplate='–î–∞—Ç–∞: %{x}<br>–í–ª–æ–∂–µ–Ω–æ: %{y:,.2f} USDT<extra></extra>'), row=1, col=1)
                
                # Add markers for ALL transactions in range (Buys and Sells)
                transactions_in_plot_range = transactions_df_raw[transactions_df_raw['date'] >= historical_prices_filtered.index.min()]
                buy_markers = transactions_in_plot_range[transactions_in_plot_range['type'] == 'buy']
                sell_markers = transactions_in_plot_range[transactions_in_plot_range['type'] == 'sell']
                
                buy_marker_times = []
                buy_marker_values = []
                buy_marker_texts = []
                for _, row in buy_markers.iterrows():
                    # Find closest index <= transaction date
                    marker_time_idx = historical_prices_filtered.index[historical_prices_filtered.index <= row['date']]
                    if not marker_time_idx.empty:
                        marker_time = marker_time_idx[-1]
                        buy_marker_times.append(marker_time)
                        buy_marker_values.append(historical_prices_filtered.loc[marker_time, 'Total_Value_Relative'])
                        buy_marker_texts.append(f"<b>–ü–æ–∫—É–ø–∫–∞ {row['asset']}</b><br>–î–∞—Ç–∞: {row['date'].strftime('%Y-%m-%d %H:%M')}<br>–ö–æ–ª-–≤–æ: {row['quantity']:.6f}<br>–¶–µ–Ω–∞: ${row['price']:.2f}<br>–°—É–º–º–∞: ${row['total_cost']:,.2f}<extra></extra>")
                if buy_marker_times:
                     fig.add_trace(go.Scatter(x=buy_marker_times, y=buy_marker_values, mode='markers', name='–ü–æ–∫—É–ø–∫–∏', marker=dict(color='#00BFFF', size=7, symbol='triangle-up', line=dict(color='white', width=1)), hoverinfo='text', text=buy_marker_texts), row=1, col=1)

                sell_marker_times = []
                sell_marker_values = []
                sell_marker_texts = []
                for _, row in sell_markers.iterrows():
                     marker_time_idx = historical_prices_filtered.index[historical_prices_filtered.index <= row['date']]
                     if not marker_time_idx.empty:
                         marker_time = marker_time_idx[-1]
                         sell_marker_times.append(marker_time)
                         sell_marker_values.append(historical_prices_filtered.loc[marker_time, 'Total_Value_Relative'])
                         sell_marker_texts.append(f"<b>–ü—Ä–æ–¥–∞–∂–∞ {row['asset']}</b><br>–î–∞—Ç–∞: {row['date'].strftime('%Y-%m-%d %H:%M')}<br>–ö–æ–ª-–≤–æ: {row['quantity']:.6f}<br>–¶–µ–Ω–∞: ${row['price']:.2f}<br>–°—É–º–º–∞: ${row['total_cost']:,.2f}<extra></extra>")
                if sell_marker_times:
                    fig.add_trace(go.Scatter(x=sell_marker_times, y=sell_marker_values, mode='markers', name='–ü—Ä–æ–¥–∞–∂–∏', marker=dict(color='#FF6347', size=7, symbol='triangle-down', line=dict(color='white', width=1)), hoverinfo='text', text=sell_marker_texts), row=1, col=1)

                # Chart 2 - Absolute P&L Stack
                num_colors = len(purchase_labels)
                colors = px.colors.qualitative.T10
                if num_colors > len(colors): colors = colors * (num_colors // len(colors)) + colors[:num_colors % len(colors)]
                color_map = {label: colors[i] for i, label in enumerate(purchase_labels)}

                for i, (pnl_col, label) in enumerate(zip(purchase_pnl_cols, purchase_labels)):
                    color = color_map[label]
                    fig.add_trace(go.Scatter(x=historical_prices_filtered.index, y=historical_prices_filtered[pnl_col].fillna(0), mode='lines', name=label, stackgroup='pnl_absolute', line=dict(width=0), fillcolor=color, hovertemplate=f'<b>{label}</b><br>–î–∞—Ç–∞: %{{x}}<br>–ê–±—Å. P&L: %{{y:,.2f}} USDT<extra></extra>', legendgroup=label, showlegend=False), row=2, col=1)
                fig.add_trace(go.Scatter(x=historical_prices_filtered.index, y=historical_prices_filtered['Total_PnL'], mode='lines', name='–û–±—â–∏–π P&L', line=dict(color='white', dash='dot', width=2), hovertemplate='<b>–û–±—â–∏–π P&L</b><br>–î–∞—Ç–∞: %{x}<br>P&L: %{y:,.2f}} USDT<extra></extra>', legendgroup="total_pnl"), row=2, col=1)
                fig.add_hline(y=0, line_width=1, line_dash="solid", line_color="grey", row=2, col=1)

                # Chart 3 - Percentage P&L Stack
                for i, (perc_contrib_col, label) in enumerate(zip(purchase_perc_contrib_cols, purchase_labels)):
                    color = color_map[label]
                    fig.add_trace(go.Scatter(x=historical_prices_filtered.index, y=historical_prices_filtered[perc_contrib_col].fillna(0), mode='lines', name=label, stackgroup='pnl_percentage', line=dict(width=0), fillcolor=color, hovertemplate=f'<b>{label}</b><br>–î–∞—Ç–∞: %{{x}}<br>% –í–∫–ª–∞–¥ P&L: %{{y:.2f}}%<extra></extra>', legendgroup=label, showlegend=False), row=3, col=1)
                fig.add_trace(go.Scatter(x=historical_prices_filtered.index, y=historical_prices_filtered['Total_PnL_Percentage'], mode='lines', name='–û–±—â–∏–π P&L %', line=dict(color='white', dash='dot', width=2), hovertemplate='<b>–û–±—â–∏–π P&L %</b><br>–î–∞—Ç–∞: %{x}<br>P&L: %{y:.2f}%<extra></extra>', legendgroup="total_pnl_perc"), row=3, col=1)
                fig.add_hline(y=0, line_width=1, line_dash="solid", line_color="grey", row=3, col=1)

                # Layout updates
                fig.update_layout(
                    height=800, hovermode='x unified',
                    legend=dict(traceorder='normal', orientation='h', yanchor='bottom', y=1.01, xanchor='right', x=1),
                    margin=dict(l=50, r=20, t=60, b=50)
                )
                fig.update_xaxes(showline=True, linewidth=1, linecolor='grey', mirror=True, gridcolor='rgba(128, 128, 128, 0.2)')
                fig.update_yaxes(showline=True, linewidth=1, linecolor='grey', mirror=True, gridcolor='rgba(128, 128, 128, 0.2)', zeroline=False)
                fig.update_yaxes(title_text="–°—Ç–æ–∏–º–æ—Å—Ç—å (USDT)", tickprefix="$", row=1, col=1)
                fig.update_yaxes(title_text="–ê–±—Å. P&L (USDT)", tickprefix="$", row=2, col=1)
                fig.update_yaxes(title_text="% –í–∫–ª–∞–¥ P&L", ticksuffix="%", row=3, col=1)
                fig.update_xaxes(title_text="–î–∞—Ç–∞", row=3, col=1)

                st.plotly_chart(fig, use_container_width=True)

    # –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞–º–∏ –∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
    elif st.session_state.active_page == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏":
        render_transactions_manager(st.session_state.username, price_data, assets)
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ app_pages.py
    elif st.session_state.active_page == "–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è":
        st.header("–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è")
        st.markdown("–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫ –±—ã –∏–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è, \\n        –µ—Å–ª–∏ –±—ã –≤—ã —Å–ª–µ–¥–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –≤–∞—à–µ–π –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π.")

        # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ê–Ω–∞–ª–∏–∑–∞ --- 
        st.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        today_date = datetime.now().date()
        default_start_date = today_date - timedelta(days=180)

        col1, col2 = st.columns(2)
        with col1:
             start_date = st.date_input("–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞", value=default_start_date, max_value=today_date - timedelta(days=1))
             commission_input = st.number_input("–ö–æ–º–∏—Å—Å–∏—è –∑–∞ —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É (%)", min_value=0.0, max_value=5.0, value=0.1, step=0.01, format="%.3f")
             rebalance_interval = st.number_input("–ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (–¥–Ω–∏)", min_value=1, value=20, step=1)
        with col2:
             end_date = st.date_input("–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞", value=today_date, min_value=start_date + timedelta(days=1))
             bank_apr_input = st.number_input("–ì–æ–¥–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ –±–∞–Ω–∫–∞ (%)", min_value=0.0, max_value=100.0, value=20.0, step=0.5, format="%.1f")
             drl_rebalance_interval = st.number_input("–ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ DRL (–¥–Ω–∏)", min_value=1, value=20, step=1)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –≤ –¥–æ–ª–∏
        commission_rate_analysis = commission_input / 100.0
        bank_apr_analysis = bank_apr_input / 100.0

        # --- –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º (–í–ê–ñ–ù–û: –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥ –≤–∞—à–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ) --- 
        # –≠—Ç–∏ –ø—É—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã –∏–∑ –º–µ—Å—Ç–∞ –∑–∞–ø—É—Å–∫–∞ Streamlit
        # –õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
        # –ü—Ä–∏–º–µ—Ä: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å os.path.dirname(__file__) –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π 
        # –ò–õ–ò –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è / –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
        # *** –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —ç—Ç–∏ –ø—É—Ç–∏! ***
        data_path_analysis = "data" # –ü—Ä–∏–º–µ—Ä: –ø–∞–ø–∫–∞ data –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
        drl_models_dir_analysis = "notebooks/trained_models" # –ü—Ä–∏–º–µ—Ä: –ø–∞–ø–∫–∞ –º–æ–¥–µ–ª–µ–π
        st.caption(f"–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {os.path.abspath(data_path_analysis)}, –ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º: {os.path.abspath(drl_models_dir_analysis)}")

        # --- –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è --- 
        user_transactions_raw = get_user_transactions(st.session_state.username)

        if not user_transactions_raw:
             st.warning("–ò—Å—Ç–æ—Ä–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏'.")
        else:
             # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π --- 
             try:
                  transactions_df_analysis = pd.DataFrame(user_transactions_raw)

                  # --- START ADDED/MODIFIED CODE ---
                  # Ensure necessary columns are numeric BEFORE calculating total_cost
                  transactions_df_analysis['quantity'] = pd.to_numeric(transactions_df_analysis['quantity'], errors='coerce')
                  transactions_df_analysis['price'] = pd.to_numeric(transactions_df_analysis['price'], errors='coerce')
                  if 'fee' in transactions_df_analysis.columns:
                       transactions_df_analysis['fee'] = pd.to_numeric(transactions_df_analysis.get('fee', 0), errors='coerce')
                  else:
                       transactions_df_analysis['fee'] = 0 # Add fee column with 0 if it doesn't exist
                  # Use assignment instead of inplace on slice
                  transactions_df_analysis['fee'] = transactions_df_analysis['fee'].fillna(0)
                  # Calculate total_cost if missing
                  if 'total_cost' not in transactions_df_analysis.columns:
                      transactions_df_analysis['total_cost'] = (transactions_df_analysis['quantity'] * transactions_df_analysis['price']) + transactions_df_analysis['fee']
                      # Handle potential NaNs from calculation (if quantity or price were NaN)
                      transactions_df_analysis['total_cost'] = transactions_df_analysis['total_cost'].fillna(0)
                  else:
                       # Ensure existing total_cost is numeric and handle NaNs
                       transactions_df_analysis['total_cost'] = pd.to_numeric(transactions_df_analysis['total_cost'], errors='coerce')
                       # Use assignment instead of inplace on slice
                       transactions_df_analysis['total_cost'] = transactions_df_analysis['total_cost'].fillna(0)
                  # Ensure fee is handled (copied from calculation block for consistency)
                  if 'fee' not in transactions_df_analysis.columns:
                       transactions_df_analysis['fee'] = 0
                  transactions_df_analysis['fee'] = pd.to_numeric(transactions_df_analysis.get('fee', 0), errors='coerce')
                  # Use assignment instead of inplace on slice
                  transactions_df_analysis['fee'] = transactions_df_analysis['fee'].fillna(0)
                  # --- END ADDED/MODIFIED CODE ---

                  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ–¥ —Ñ–æ—Ä–º–∞—Ç run_portfolio_analysis
                  rename_map = {
                       "date": "–î–∞—Ç–∞_–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏",
                       "asset": "–ê–∫—Ç–∏–≤",
                       "type": "–¢–∏–ø",
                       "quantity": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
                       "total_cost": "–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å"
                  }
                  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–∫–ª—é—á–µ–π –∏–∑ rename_map) –≤ DataFrame
                  required_cols = list(rename_map.keys())
                  missing_cols = [col for col in required_cols if col not in transactions_df_analysis.columns]
                  if missing_cols:
                      st.error(f"–û—à–∏–±–∫–∞: –ù–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {missing_cols}")
                  else:
                     # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ü–ï–†–ï–î –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º
                     transactions_df_analysis = transactions_df_analysis[required_cols].rename(columns=rename_map)

                     # --- START ADDED CODE ---
                     # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–µ '–¢–∏–ø'
                     type_value_map = {'buy': '–ü–æ–∫—É–ø–∫–∞', 'sell': '–ü—Ä–æ–¥–∞–∂–∞'}
                     # –ò—Å–ø–æ–ª—å–∑—É–µ–º .get –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞, –æ—Å—Ç–∞–≤–ª—è—è –¥—Ä—É–≥–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                     transactions_df_analysis['–¢–∏–ø'] = transactions_df_analysis['–¢–∏–ø'].map(lambda x: type_value_map.get(x, x))
                     # --- END ADDED CODE ---

                     # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
                     transactions_df_analysis['–î–∞—Ç–∞_–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏'] = pd.to_datetime(transactions_df_analysis['–î–∞—Ç–∞_–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏'], errors='coerce')
                     transactions_df_analysis['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] = pd.to_numeric(transactions_df_analysis['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'], errors='coerce')
                     # –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å NaN –¥–ª—è –ø—Ä–æ–¥–∞–∂, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                     transactions_df_analysis['–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å'] = pd.to_numeric(transactions_df_analysis['–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å'].astype(str).str.replace(',', '', regex=False).str.replace('$', '', regex=False), errors='coerce')
                     
                     transactions_df_analysis.dropna(subset=['–î–∞—Ç–∞_–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏', '–ê–∫—Ç–∏–≤', '–¢–∏–ø'], inplace=True)
                     # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                     if transactions_df_analysis.empty:
                          st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å—Ç–æ—Ä–∏–∏.")
                     else:
                          st.dataframe(transactions_df_analysis.head(), use_container_width=True)

                          # --- –ö–Ω–æ–ø–∫–∞ –ó–∞–ø—É—Å–∫–∞ --- 
                          if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è", use_container_width=True):
                              # Clear previous results from session state
                              if 'analysis_results' in st.session_state: del st.session_state['analysis_results']
                              
                              with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è."):
                                  try:
                                     # Call the analysis function, it now returns a dictionary
                                     analysis_output = run_portfolio_analysis(
                                         transactions_df=transactions_df_analysis,
                                         start_date_str=start_date.strftime('%Y-%m-%d'),
                                         end_date_str=end_date.strftime('%Y-%m-%d'),
                                         data_path=data_path_analysis,
                                         drl_models_dir=drl_models_dir_analysis,
                                         bank_apr=bank_apr_analysis,
                                         commission_rate=commission_rate_analysis,
                                         rebalance_interval_days=rebalance_interval,
                                         drl_rebalance_interval_days=drl_rebalance_interval
                                     )
                                     
                                     # Store the entire results dictionary in session state
                                     st.session_state.analysis_results = analysis_output
                                     
                                     # Check for errors returned by the function
                                     if analysis_output.get("error"):
                                          st.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {analysis_output['error']}")
                                     elif analysis_output.get("figure") is None or analysis_output.get("metrics_display").empty:
                                          st.error("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è, –Ω–æ –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–≥—Ä–∞—Ñ–∏–∫ –∏–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—É—Å—Ç—ã).")
                                     else:
                                          st.success("–ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                                          # Rerun to display results correctly
                                          st.rerun()
                                          
                                  except Exception as e:
                                      st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞: {e}")
                                      traceback.print_exc()
                                      # Clear results on critical error
                                      if 'analysis_results' in st.session_state: del st.session_state['analysis_results']
             except Exception as e:
                 st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {e}")
                 traceback.print_exc()

        if st.session_state.get('analysis_in_progress', False):
            st.warning("–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")
            st.progress(st.session_state.get('analysis_progress', 0.0))
            if 'analysis_status' in st.session_state:
                st.text(st.session_state.analysis_status)
        else:
            # Display results if available and no error
            if 'analysis_results' in st.session_state and not st.session_state.analysis_results.get("error"):
                results_dict = st.session_state.analysis_results
                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ê–Ω–∞–ª–∏–∑–∞ –ü–æ—Ä—Ç—Ñ–µ–ª—è")

                # Display Plotly figure if it exists
                fig_analysis = results_dict.get("figure")
                if fig_analysis:
                    st.plotly_chart(fig_analysis, use_container_width=True, key="portfolio_analysis_figure") 
                else:
                    st.warning("–ì—Ä–∞—Ñ–∏–∫ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω.") 

                # Display Metrics (using the formatted display DataFrame)
                st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
                metrics_display_df = results_dict.get('metrics_display', pd.DataFrame())
                if not metrics_display_df.empty:
                    st.dataframe(metrics_display_df, use_container_width=True) # Display the pre-formatted df
                else:
                     st.warning("–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã.") 

                # ----- NEW: Chatbot Section -----
                st.divider() # Add a visual separator
                st.subheader("–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –æ –ø–æ—Ä—Ç—Ñ–µ–ª–µ (AI –ê–≥–µ–Ω—Ç)")

                # Initialize chat history in session state if it doesn't exist
                if "messages" not in st.session_state:
                    st.session_state.messages = []

                # Display chat messages from history on app rerun
                chat_container = st.container(height=300)
                with chat_container:
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.markdown(message["content"])

                # React to user input
                if prompt := st.chat_input("–°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞..."):
                    # Display user message in chat message container
                    with chat_container:
                         with st.chat_message("user"):
                             st.markdown(prompt)
                    # Add user message to chat history
                    st.session_state.messages.append({"role": "user", "content": prompt})

                    # ---- Get AI response ----
                    response = "" # Initialize response variable
                    try:
                        # 1. Check if analysis results exist
                        if 'analysis_results' not in st.session_state or not st.session_state.analysis_results:
                            response = "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è, —á—Ç–æ–±—ã —è –º–æ–≥ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã."
                        else:
                            # 2. Initialize the agent (cached)
                            agent = initialize_finrobot_agent()
                            if agent is None:
                                response = "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AI-–∞–≥–µ–Ω—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ API –∫–ª—é—á–∏."
                            else:
                                # 3. Format the analysis data for the LLM
                                analysis_summary = format_portfolio_data_for_llm(st.session_state.analysis_results)
                                if not analysis_summary or analysis_summary == "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.":
                                     response = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
                                else:
                                     # 4. Construct the full prompt
                                     full_prompt = f"""
                                     –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ä—Ç—Ñ–µ–ª—è:
                                     ```markdown
                                     {analysis_summary}
                                     ```

                                     –û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å **—Ç–æ–ª—å–∫–æ** –Ω–∞ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
                                     '{prompt}'
                                     """

                                     # 5. Call the agent
                                     with st.spinner("ü§ñ AI-–∞–≥–µ–Ω—Ç –¥—É–º–∞–µ—Ç..."):
                                          # FinRobot's SingleAssistant might return the response directly
                                          # or store it in the conversation history. Adjust based on its behavior.
                                          # Assuming agent.chat returns the final message content:
                                          agent_reply = agent.chat(full_prompt)
                                          # Extract the actual response content if needed (depends on agent's return format)
                                          # This might need adjustment based on how SingleAssistant returns replies
                                          if isinstance(agent_reply, dict) and 'content' in agent_reply:
                                              response = agent_reply['content']
                                          elif isinstance(agent_reply, str):
                                               response = agent_reply
                                          else:
                                               # Fallback: Try to get the last message from the agent's history
                                               if hasattr(agent, 'agent') and hasattr(agent.agent, 'chat_messages') and agent.agent.chat_messages:
                                                    last_msg = agent.agent.chat_messages.get(agent.agent.opponent, [])[-1]
                                                    response = last_msg.get('content', "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞.")
                                               else:
                                                   response = "–ü–æ–ª—É—á–µ–Ω –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞–≥–µ–Ω—Ç–∞."

                    except Exception as e:
                        response = f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ AI: {e}"
                        traceback.print_exc()

                    # ---- Display AI response ----
                    with chat_container:
                         with st.chat_message("assistant"):
                            st.markdown(response)
                    # Add assistant response to chat history
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    # Rerun to ensure the message list is updated cleanly in the container
                    st.rerun()

            # Display error if analysis failed
            elif 'analysis_results' in st.session_state and st.session_state.analysis_results.get("error"):
                 st.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {st.session_state.analysis_results['error']}")
            else:
                 st.info("–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.") 

    # --- End Section: Portfolio Analysis ---

    # –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞–º–∏ –∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
    elif st.session_state.active_page == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏":
        render_transactions_manager(st.session_state.username, price_data, assets)
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ app_pages.py
    elif st.session_state.active_page == "Dashboard":
        render_dashboard(st.session_state.username, price_data, model_returns, model_actions, assets)
    
    elif st.session_state.active_page == "Portfolio Optimization":
        render_portfolio_optimization(st.session_state.username, price_data, assets)
    
    elif st.session_state.active_page == "Model Training":
        render_model_training(st.session_state.username, price_data, assets)
    
    elif st.session_state.active_page == "Model Comparison":
        render_model_comparison(st.session_state.username, model_returns, model_actions, price_data)
    
    elif st.session_state.active_page == "Backtest Results":
        render_backtest(st.session_state.username, model_returns, price_data)
    
    elif st.session_state.active_page == "About":
        render_about()

    # <<< Add block for the new Recommendations page >>>
    elif st.session_state.active_page == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏":
        st.header("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è (DRL)")
        st.markdown("–í—ã–±–µ—Ä–∏—Ç–µ DRL-—Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–æ–≤ –∏–∑ **–Ω–∞–±–æ—Ä–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ–±—É—á–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å.**")

        # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ---
        st.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã")
        drl_model_names = ["A2C", "PPO", "SAC", "DDPG"]
        
        # Remove columns and number input, keep only model selection
        selected_model_name = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ DRL –º–æ–¥–µ–ª—å:", drl_model_names, key="drl_model_select")
        # Remove the number input for rebalancing interval
        # reco_rebalance_interval = st.number_input(...)

        # --- Define DRL training assets --- #
        # !! –í–∞–∂–Ω–æ: –≠—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ –î–û–õ–ñ–ï–ù —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –∞–∫—Ç–∏–≤–∞–º–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π !!
        DRL_TRAINING_ASSETS_RECO = ['APTUSDT', 'CAKEUSDT', 'HBARUSDT', 'JUPUSDT', 'PEPEUSDT', 'STRKUSDT', 'USDCUSDT']
        st.info(f"–ú–æ–¥–µ–ª—å DRL –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –∞–∫—Ç–∏–≤–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –Ω–∏–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ü–µ–ª–µ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ **—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ö –∞–∫—Ç–∏–≤–æ–≤ –∏–∑ —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å —É –≤–∞—Å —Å–µ–π—á–∞—Å**, –ø–ª—é—Å **{STABLECOIN_ASSET}** –∫–∞–∫ —Ç–∏—Ö—É—é –≥–∞–≤–∞–Ω—å.")

        data_path_reco = "data"
        drl_models_dir_reco = "notebooks/trained_models"
        st.caption(f"–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {os.path.abspath(data_path_reco)}, –ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º: {os.path.abspath(drl_models_dir_reco)}")

        # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if st.button(f"–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –æ—Ç {selected_model_name}", use_container_width=True):
            with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ {selected_model_name}..."):
                try:
                    # --- 1. Get Current Portfolio Value --- #
                    portfolio_data_reco = get_portfolio_with_quantities(st.session_state.username)
                    total_portfolio_value = 0
                    if portfolio_data_reco and any(portfolio_data_reco["quantities"].values()):
                        # Use price_data loaded globally for the app
                        latest_prices_reco = price_data.iloc[-1] if not price_data.empty else None
                        if latest_prices_reco is not None:
                            for asset, quantity in portfolio_data_reco["quantities"].items():
                                if quantity > 0 and asset in latest_prices_reco.index:
                                    total_portfolio_value += quantity * latest_prices_reco[asset]
                        else:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã –∞–∫—Ç–∏–≤–æ–≤ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ–±—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è.")
                            st.stop()
                    if total_portfolio_value < 1e-6:
                        st.warning("–¢–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è —Ä–∞–≤–Ω–∞ –Ω—É–ª—é –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –µ–µ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
                        st.stop()

                    st.write(f"–¢–µ–∫—É—â–∞—è –æ–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è: ${total_portfolio_value:,.2f}")

                    # --- 2. Load Latest Data for DRL Assets --- #
                    # Determine lookback period (max indicator window + cov lookback)
                    # Example: Max window = 60 (SMA), Cov lookback = 24 -> need ~84 hours + buffer
                    lookback_days_data = 5 # Load last 5 days of hourly data (adjust as needed)
                    end_date_data = datetime.now()
                    start_date_data = end_date_data - timedelta(days=lookback_days_data)

                    all_drl_data_frames = []
                    print(f"Loading latest data for {len(DRL_TRAINING_ASSETS_RECO)} DRL assets...")
                    print(f"Filtering data between: {start_date_data} and {end_date_data}")
                    for asset in DRL_TRAINING_ASSETS_RECO:
                        filepath = os.path.join(data_path_reco, f"{asset}_hourly_data.csv")
                        df_asset = preprocess_asset_data(filepath, asset, STABLECOIN_ASSET)
                        if not df_asset.empty:
                             print(f"  {asset}: Loaded {len(df_asset)} rows. Date range: {df_asset['date'].min()} to {df_asset['date'].max()}")
                             df_asset_filtered = df_asset[(df_asset['date'] >= start_date_data) & (df_asset['date'] <= end_date_data)]
                             print(f"  {asset}: Filtered to {len(df_asset_filtered)} rows.")
                             if not df_asset_filtered.empty:
                                all_drl_data_frames.append(df_asset_filtered)
                        else:
                              st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è DRL –∞–∫—Ç–∏–≤–∞: {asset}. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
                              st.stop()

                    if not all_drl_data_frames:
                         st.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DRL –∞–∫—Ç–∏–≤–æ–≤ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ {start_date_data.strftime('%Y-%m-%d')} - {end_date_data.strftime('%Y-%m-%d')}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å CSV —Ñ–∞–π–ª–æ–≤.")
                         st.stop()

                    latest_drl_data = pd.concat(all_drl_data_frames, ignore_index=True)

                    # --- 3. Preprocess Data (Features + Covariance) --- #
                    print("Preprocessing latest data (FeatureEngineer)...")
                    fe = FeatureEngineer(use_technical_indicator=True, tech_indicator_list=INDICATORS)
                    drl_processed = fe.preprocess_data(latest_drl_data.copy()) # Pass copy
                    # Note: fe.preprocess_data now includes dropna() based on previous steps
                    if drl_processed.empty:
                        st.error("–û—à–∏–±–∫–∞ FeatureEngineer: DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ dropna(). –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")
                        st.stop()

                    print("Calculating latest covariance matrix...")
                    lookback_cov = 24 # Standard lookback for covariance
                    drl_processed = drl_processed.sort_values(['date', 'tic'])
                    last_date_available = drl_processed['date'].max()
                    # Get data for the last lookback_cov hours ending at the last available date
                    cov_data_input = drl_processed[drl_processed['date'] > (last_date_available - timedelta(hours=lookback_cov))].copy()
                    if len(cov_data_input['date'].unique()) < lookback_cov / 2: # Basic check for enough data
                         st.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(cov_data_input['date'].unique())} —á–∞—Å–æ–≤) –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ (—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–∫–æ–ª–æ {lookback_cov} —á–∞—Å–æ–≤).")
                         st.stop()

                    price_lookback = cov_data_input.pivot_table(index='date', columns='tic', values='close')
                    price_lookback = price_lookback.reindex(columns=DRL_TRAINING_ASSETS_RECO, fill_value=np.nan)
                    return_lookback = price_lookback.pct_change().dropna(how='all')
                    latest_cov_matrix = np.zeros((len(DRL_TRAINING_ASSETS_RECO), len(DRL_TRAINING_ASSETS_RECO)))
                    if len(return_lookback) >= len(DRL_TRAINING_ASSETS_RECO):
                         return_lookback_valid = return_lookback.dropna(axis=1, how='all')
                         if not return_lookback_valid.empty and return_lookback_valid.shape[1] > 1:
                              latest_cov_matrix = return_lookback_valid.cov().reindex(index=DRL_TRAINING_ASSETS_RECO, columns=DRL_TRAINING_ASSETS_RECO).fillna(0).values

                    # --- 4. Construct Latest State --- #
                    print("Constructing latest observation state...")
                    last_processed_data = drl_processed[drl_processed['date'] == last_date_available]
                    if last_processed_data.empty:
                         st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                         st.stop()
                    # Ensure data is sorted by the canonical asset list order for consistency
                    last_processed_data = last_processed_data.set_index('tic').reindex(DRL_TRAINING_ASSETS_RECO).reset_index()

                    # Extract indicators for the last timestamp for all DRL assets
                    indicator_values = last_processed_data[INDICATORS].values.T # Shape (num_indicators, num_assets)
                    # Combine cov matrix and indicators
                    latest_state = np.append(latest_cov_matrix, indicator_values, axis=0) # Shape (stock_dim + num_ind, stock_dim)
                    # Ensure state shape matches expected (15, 7)
                    expected_shape = (len(DRL_TRAINING_ASSETS_RECO) + len(INDICATORS), len(DRL_TRAINING_ASSETS_RECO))
                    if latest_state.shape != expected_shape:
                        st.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è: –ø–æ–ª—É—á–µ–Ω–æ {latest_state.shape}, –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_shape}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã.")
                        st.stop()

                    # --- 5. Load Selected Model --- #
                    print(f"Loading model {selected_model_name}...")
                    model_file = os.path.join(drl_models_dir_reco, f"trained_{selected_model_name.lower()}.zip")
                    if not os.path.exists(model_file):
                         st.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_file}")
                         st.stop()
                    model_class_reco = globals()[selected_model_name] # Get class by name
                    model = model_class_reco.load(model_file)

                    # --- 6. Predict Action --- #
                    print("Predicting action...")
                    # Model expects a batch dimension, add it if predicting single state
                    # latest_state_batch = np.expand_dims(latest_state, axis=0)
                    # However, stable-baselines3 predict usually handles single obs automatically
                    raw_actions, _ = model.predict(latest_state, deterministic=True)

                    # --- 7. Normalize Weights --- #
                    print("Normalizing weights...")
                    target_weights_raw = softmax_normalization(raw_actions)
                    target_weights_dict_raw = {asset: weight for asset, weight in zip(DRL_TRAINING_ASSETS_RECO, target_weights_raw)}

                    # --- 8. Filter, Renormalize, Calculate Target Values & Display --- #
                    st.markdown("--- ")
                    st.subheader(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –æ—Ç {selected_model_name} (–¥–ª—è —Ç–µ–∫—É—â–∏—Ö –∞–∫—Ç–∏–≤–æ–≤ + {STABLECOIN_ASSET})")
                    
                    # Remove the informational text about the rebalancing interval
                    # st.info(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ç–µ–∫—É—â–∏—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. \n–ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É —á–µ—Ä–µ–∑ **{reco_rebalance_interval}** –¥–Ω–µ–π.")

                    # Get current user assets (only those also in DRL training set)
                    current_user_assets_in_drl_set = set()
                    if portfolio_data_reco and any(portfolio_data_reco["quantities"].values()):
                         current_user_assets_in_drl_set = {
                              asset for asset, quantity in portfolio_data_reco["quantities"].items()
                              if quantity > 1e-9 and asset in DRL_TRAINING_ASSETS_RECO
                         }

                    # Define the set of assets to consider for the final recommendation
                    relevant_assets = current_user_assets_in_drl_set.copy()
                    relevant_assets.add(STABLECOIN_ASSET) # Always include stablecoin

                    # Filter the raw weights to include only relevant assets
                    filtered_weights = {asset: target_weights_dict_raw.get(asset, 0.0)
                                        for asset in relevant_assets}

                    # Renormalize the filtered weights
                    total_filtered_weight = sum(filtered_weights.values())
                    final_target_weights = {}
                    if total_filtered_weight > 1e-9:
                        final_target_weights = {asset: weight / total_filtered_weight
                                                for asset, weight in filtered_weights.items()}
                    elif relevant_assets: # If sum is zero, but we have assets, distribute equally
                        print("Warning: Sum of filtered weights is zero. Distributing equally among relevant assets.")
                        num_relevant = len(relevant_assets)
                        final_target_weights = {asset: 1.0 / num_relevant for asset in relevant_assets}
                    else: # Should not happen if stablecoin is always added
                         st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∞–∫—Ç–∏–≤—ã –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.")
                         st.stop()

                    st.markdown(f"–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–ø–∏—Ç–∞–ª–∞ (${total_portfolio_value:,.2f}) –º–µ–∂–¥—É **–≤–∞—à–∏–º–∏ –∞–∫—Ç–∏–≤–∞–º–∏ –∏–∑ –Ω–∞–±–æ—Ä–∞ DRL –∏ {STABLECOIN_ASSET}**: ")

                    # Calculate final target values
                    results_list = []
                    for asset, weight in final_target_weights.items():
                        target_value = total_portfolio_value * weight
                        results_list.append({
                            "–ê–∫—Ç–∏–≤": asset,
                            "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤–µ—Å (%)": weight * 100,
                            "–¶–µ–ª–µ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å ($": target_value
                        })

                    results_df_reco = pd.DataFrame(results_list)
                    results_df_reco = results_df_reco.sort_values(by="–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤–µ—Å (%)", ascending=False).reset_index(drop=True)

                    # Display table
                    st.dataframe(results_df_reco.style.format({
                        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤–µ—Å (%)": "{:.2f}%",
                        "–¶–µ–ª–µ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å ($": "${:,.2f}"
                    }))

                    # Optional: Display as bar chart
                    st.subheader("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤")
                    # Use the final filtered/renormalized weights for the chart
                    chart_data = results_df_reco.set_index("–ê–∫—Ç–∏–≤")["–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤–µ—Å (%)"] / 100.0
                    st.bar_chart(chart_data)

                    st.success("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞!")

                except Exception as e:
                    st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
                    traceback.print_exc()
        pass # End of Recommendations page block

    # <<< Add block for the new Data & Analysis page >>>
    elif st.session_state.active_page == "–î–∞–Ω–Ω—ã–µ –∏ –ê–Ω–∞–ª–∏–∑":
        st.header("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ –∏ –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞")
        st.markdown("–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∞–∫—Ç–∏–≤–æ–≤ —Å Binance –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞.")

        # --- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö --- #
        st.subheader("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∞–∫—Ç–∏–≤–æ–≤")
        st.warning("**–í–Ω–∏–º–∞–Ω–∏–µ:** –î–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å Binance –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å API –∫–ª—é—á–∏ –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö Streamlit –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è (`BINANCE_API_KEY`, `BINANCE_API_SECRET`).")

        # Initialize update status if not present
        if 'last_update_status' not in st.session_state:
            st.session_state.last_update_status = None
        if 'last_update_time' not in st.session_state:
            st.session_state.last_update_time = None
        if 'update_counter' not in st.session_state: # For cache invalidation
            st.session_state.update_counter = 0

        col1_update, col2_update = st.columns([3, 1])
        with col1_update:
            # Rename the button
            if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", use_container_width=True):
                st.session_state.last_update_status = None # Reset status
                st.session_state.last_update_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                progress_bar = st.progress(0, text="–ó–∞–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è...")
                try:
                    success_update, message_update = update_all_asset_data(progress_bar=progress_bar)
                    st.session_state.last_update_status = message_update
                    if success_update:
                        progress_bar.progress(1.0, text="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
                        # If update was successful, recreate combined file
                        success_combine, message_combine = create_combined_data()
                        if success_combine:
                            st.session_state.last_update_status += f" {message_combine}"
                            st.session_state.update_counter += 1 # Increment counter to invalidate cache
                            st.success(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! {message_combine}")
                            st.rerun() # Rerun to reload data with new trigger
                        else:
                            st.session_state.last_update_status += f" –û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {message_combine}"
                            st.error(f"–§–∞–π–ª—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {message_combine}")
                    else:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ CSV —Ñ–∞–π–ª–æ–≤: {message_update}")
                    progress_bar.empty() # Remove progress bar
                except Exception as e:
                    st.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
                    traceback.print_exc()
                    st.session_state.last_update_status = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}"
                    progress_bar.empty()

        # Display last update status
        if st.session_state.last_update_status:
             st.info(f"–°—Ç–∞—Ç—É—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è ({st.session_state.last_update_time}): {st.session_state.last_update_status}")

        st.markdown("--- ")
        st.subheader("–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞")

        # Load data using the cached function with the trigger
        # Display loading spinner while data is loading via cache
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö..."):
             combined_df = load_combined_data_cached(st.session_state.update_counter)

        if not combined_df.empty:
            st.dataframe(combined_df.tail()) # Show recent combined data tail

            # --- Normalized Plot Section ---
            st.markdown("#### –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Ü–µ–Ω")
            # Add period selection for normalized plot
            norm_period_options = {"7 –¥–Ω–µ–π": 7, "30 –¥–Ω–µ–π": 30, "90 –¥–Ω–µ–π": 90, "180 –¥–Ω–µ–π": 180, "–í—Å–µ –≤—Ä–µ–º—è": None}
            selected_norm_period_label = st.radio(
                "–ü–µ—Ä–∏–æ–¥ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:", 
                options=norm_period_options.keys(), 
                index=3, # Default to 180 days
                horizontal=True, 
                key="norm_period_radio"
            )
            selected_norm_days = norm_period_options[selected_norm_period_label]

            try:
                with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞..."):
                     # Pass selected days to the plotting function
                     fig_norm = generate_normalized_plot(combined_df, days=selected_norm_days)
                     st.plotly_chart(fig_norm, use_container_width=True)
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ü–µ–Ω: {e}")

            # --- Correlation Heatmap Section ---
            st.markdown("#### –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π") # Adjusted title slightly
            # Add period selection for correlation heatmap
            corr_period_options = {
                "1 —á–∞—Å": "h", "1 –¥–µ–Ω—å": "D", "3 –¥–Ω—è": "3D", "1 –Ω–µ–¥–µ–ª—è": "W", 
                "1 –º–µ—Å—è—Ü": "MS", "3 –º–µ—Å—è—Ü–∞": "3MS", "1 –≥–æ–¥": "YS"
            }
            selected_corr_period_label = st.radio(
                "–ü–µ—Ä–∏–æ–¥ —Ä–∞—Å—á–µ—Ç–∞ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:", 
                options=corr_period_options.keys(), 
                index=3, # Default to 1 –Ω–µ–¥–µ–ª—è
                horizontal=True, 
                key="corr_period_radio"
            )
            selected_corr_freq = corr_period_options[selected_corr_period_label]
            
            try:
                with st.spinner(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π ({selected_corr_period_label})..."):
                     # Pass selected frequency to the plotting function
                     fig_corr = generate_correlation_heatmap(combined_df, frequency=selected_corr_freq)
                     st.plotly_chart(fig_corr, use_container_width=True)
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")

            # --- NEW: Single Asset Plot Section --- 
            st.markdown("--- ")
            st.subheader("–ì—Ä–∞—Ñ–∏–∫ —Ü–µ–Ω—ã –∞–∫—Ç–∏–≤–∞")

            if not combined_df.empty:
                # Get available assets from combined_df columns
                available_assets = combined_df.columns.tolist()

                col1_asset, col2_res = st.columns([2,3])
                with col1_asset:
                    selected_asset = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∞–∫—Ç–∏–≤:", available_assets, key="single_asset_select")
                with col2_res:
                    resolution_options = {"1 —á–∞—Å": "h", "4 —á–∞—Å–∞": "4h", "1 –¥–µ–Ω—å": "D", "1 –Ω–µ–¥–µ–ª—è": "W", "1 –º–µ—Å—è—Ü": "MS"}
                    selected_resolution_label = st.radio("–¢–∞–π–º—Ñ—Ä–µ–π–º:", 
                                                       options=resolution_options.keys(), 
                                                       index=0, # Default to 1h
                                                       horizontal=True, 
                                                       key="resolution_radio"
                                                    )
                    selected_resolution_code = resolution_options[selected_resolution_label]
                
                # Generate and display the single asset plot
                if selected_asset:
                    try:
                         with st.spinner(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ {selected_asset} ({selected_resolution_label})..."):
                              # Import the new function if not already imported at the top
                              from portfolios_optimization.data_loader import generate_single_asset_plot 
                              
                              # Call the function to generate the plot
                              fig_single = generate_single_asset_plot(combined_df, selected_asset, selected_resolution_code)
                              st.plotly_chart(fig_single, use_container_width=True)
                              # Remove the placeholder info message
                              # st.info(f"–õ–æ–≥–∏–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ {selected_asset} —Å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–º {selected_resolution_label} –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞.") 
                    except Exception as e:
                         st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è {selected_asset}: {e}")
                         traceback.print_exc() # Print detailed error in console

        else:
             st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ 'data/data_compare_eda.csv'. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö.")
        pass # End of Data & Analysis page block

    # <<< Add block for the new Research page >>>
    elif st.session_state.active_page == "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ":
        st.header("–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ä—Ç—Ñ–µ–ª–µ–π")
        st.markdown("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ, –∫–∞–∫ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –±—ã —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è, –µ—Å–ª–∏ –±—ã –≤—ã –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ \n        —É–∫–∞–∑–∞–Ω–Ω—É—é **–Ω–∞—á–∞–ª—å–Ω—É—é —Å—É–º–º—É** –≤ **–≤—ã–±—Ä–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∞–∫—Ç–∏–≤–æ–≤** –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.") # Modified description

        # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è --- #
        st.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∏–º—É–ª—è—Ü–∏–∏")
        
        # Load combined data to get available assets (cached)
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–∫—Ç–∏–≤–æ–≤..."):
             # Use the same counter as Data & Analysis tab for cache consistency
             combined_df_research = load_combined_data_cached(st.session_state.get('update_counter', 0)) 

        if combined_df_research.empty:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ ('data/data_compare_eda.csv'). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        else:
            available_assets_research = combined_df_research.columns.tolist()
            # Exclude stablecoin from default selection if present
            default_risky = [a for a in available_assets_research if a != STABLECOIN_ASSET]
            assets_options = available_assets_research
            
            selected_hypothetical_assets = st.multiselect(
                "–í—ã–±–µ—Ä–∏—Ç–µ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –∞–∫—Ç–∏–≤–æ–≤ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏:",
                options=assets_options,
                # Select first 5 risky assets by default
                default=default_risky[:min(len(default_risky), 5)], 
                key="research_asset_select",
                help=f"–í—ã–±–µ—Ä–∏—Ç–µ –∞–∫—Ç–∏–≤—ã –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ —Å–∏–º—É–ª—è—Ü–∏—é. {STABLECOIN_ASSET} –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤."
            )
            
            # --- NEW: Initial Investment Input --- 
            initial_investment_amount = st.number_input(
                "–ù–∞—á–∞–ª—å–Ω–∞—è —Å—É–º–º–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π (USD):", 
                min_value=1.0, 
                value=10000.0, 
                step=100.0, 
                format="%.2f",
                key="research_initial_investment",
                help="–°—É–º–º–∞, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—á–Ω–µ—Ç—Å—è —Å–∏–º—É–ª—è—Ü–∏—è."
            )
            # --- End Initial Investment Input ---
            
            st.markdown("**–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞:**")
            with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å/—Å–∫—Ä—ã—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞"):
                today_date = datetime.now().date()
                default_start_date = today_date - timedelta(days=365) 
                
                col1_params, col2_params = st.columns(2)
                with col1_params:
                     sim_start_date = st.date_input("–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ —Å–∏–º—É–ª—è—Ü–∏–∏", value=default_start_date, max_value=today_date - timedelta(days=1), key="research_start_date")
                     sim_commission = st.number_input("–ö–æ–º–∏—Å—Å–∏—è –∑–∞ —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É (%) [0.1% = 0.001]", min_value=0.0, max_value=5.0, value=0.1, step=0.01, format="%.3f", key="research_commission")
                     sim_rebalance_interval = st.number_input("–ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (–¥–Ω–∏)", min_value=1, value=20, step=1, key="research_rebalance_interval", help="–î–ª—è Equal Weight, Markowitz, Oracle")
                with col2_params:
                     sim_end_date = st.date_input("–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ —Å–∏–º—É–ª—è—Ü–∏–∏", value=today_date, min_value=sim_start_date + timedelta(days=1) if sim_start_date else None, key="research_end_date")
                     sim_bank_apr = st.number_input("–ì–æ–¥–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ –±–∞–Ω–∫–∞ (%) [20% = 0.2]", min_value=0.0, max_value=100.0, value=20.0, step=0.5, format="%.1f", key="research_bank_apr")
                     sim_drl_rebalance_interval = st.number_input("–ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ DRL (–¥–Ω–∏)", min_value=1, value=20, step=1, key="research_drl_interval", help="–î–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π A2C, PPO, SAC, DDPG")
                
                sim_commission_rate = sim_commission / 100.0
                sim_bank_apr_rate = sim_bank_apr / 100.0

            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", use_container_width=True, key="research_run_button"):
                if not selected_hypothetical_assets:
                     st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∞–∫—Ç–∏–≤ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏.")
                elif not sim_start_date or not sim_end_date:
                     st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–∞—á–∞–ª—å–Ω—É—é –∏ –∫–æ–Ω–µ—á–Ω—É—é –¥–∞—Ç—ã.")
                elif sim_end_date <= sim_start_date:
                     st.warning("–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ–∑–∂–µ –Ω–∞—á–∞–ª—å–Ω–æ–π.")
                else:
                     st.session_state['research_results'] = None 
                     st.session_state['research_figure'] = None
                     
                     # --- REMOVED User transaction fetching --- 
                     # user_transactions = get_user_transactions(st.session_state.username)
                     # if not user_transactions: ... 
                     
                     # Define data and model paths (ensure they are correct)
                     # These should ideally be relative or configured
                     data_path_research = "data"
                     drl_models_dir_research = "notebooks/trained_models"
                     st.caption(f"–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {os.path.abspath(data_path_research)}, –ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º DRL: {os.path.abspath(drl_models_dir_research)}")

                     with st.spinner("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∏–º—É–ª—è—Ü–∏–∏... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è."):
                         try:
                             # --- UPDATED Call to the analysis function --- 
                             results_summary_hypo, fig_hypo = run_hypothetical_analysis(
                                 # user_transactions_list=user_transactions, # REMOVED
                                 initial_investment=initial_investment_amount, # ADDED
                                 hypothetical_assets=selected_hypothetical_assets,
                                 start_date_str=sim_start_date.strftime('%Y-%m-%d'),
                                 end_date_str=sim_end_date.strftime('%Y-%m-%d'),
                                 data_path=data_path_research, # Pass data path
                                 bank_apr=sim_bank_apr_rate,
                                 commission_rate=sim_commission_rate,
                                 rebalance_interval_days=sim_rebalance_interval,
                                 drl_rebalance_interval_days=sim_drl_rebalance_interval,
                                 drl_models_dir=drl_models_dir_research # Pass DRL model path
                             )
                             # --- End Updated Call ---
                             
                             if results_summary_hypo is not None and fig_hypo is not None:
                                  st.session_state['research_results'] = results_summary_hypo
                                  st.session_state['research_figure'] = fig_hypo
                                  st.success("–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!", icon="‚úÖ")
                             else:
                                  # Error messages should come from run_hypothetical_analysis
                                  st.error("–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ –≤—ã—à–µ –∏–ª–∏ –≤ –∫–æ–Ω—Å–æ–ª–∏.")
                         except Exception as e:
                             st.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
                             traceback.print_exc()

            st.markdown("--- ")
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
            # Display logic remains mostly the same
            if 'research_results' in st.session_state and st.session_state.research_results is not None:
                 # Display the DataFrame which now contains formatted metrics
                 st.dataframe(st.session_state.research_results, use_container_width=True)
            
            if 'research_figure' in st.session_state and st.session_state.research_figure is not None:
                 st.plotly_chart(st.session_state.research_figure, use_container_width=True)
            # Modify the info message slightly
            elif 'research_run_button' not in st.session_state or not st.session_state.get('research_run_button', False):
                 st.info("–í—ã–±–µ—Ä–∏—Ç–µ –∞–∫—Ç–∏–≤—ã, —É–∫–∞–∂–∏—Ç–µ –Ω–∞—á–∞–ª—å–Ω—É—é —Å—É–º–º—É –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∑–∞—Ç–µ–º –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É '–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ' –≤—ã—à–µ.")
            elif st.session_state.get('research_results') is None and st.session_state.get('research_figure') is None:
                 # This case means the button was clicked, but the analysis failed or returned None
                 st.info("–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –±—ã–ª–æ –∑–∞–ø—É—â–µ–Ω–æ, –Ω–æ –Ω–µ –≤–µ—Ä–Ω—É–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö –≤—ã—à–µ.")
                 
        pass # End of Research page block
    
    # --- Add a final else or elif for any remaining page options or a fallback ---
    # Example: Check if any other page option from page_options was selected
    # This handles the case where the last 'elif' doesn't have a following block
    # Add elif blocks for any remaining pages like "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏", etc.
    elif st.session_state.active_page == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞–º–∏":
        render_transactions_manager(st.session_state.username, price_data, assets)
    elif st.session_state.active_page == "–ï–¥–∏–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç":
        # ... (Code for this page)
        pass # Make sure this page also ends correctly
    elif st.session_state.active_page == "–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è":
        # ... (Code for this page)
        pass # Make sure this page also ends correctly
    elif st.session_state.active_page == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏":
        # ... (Code for this page)
        pass # Make sure this page also ends correctly
    else:
        # Fallback if active_page is somehow invalid 
        st.error("–í—ã–±—Ä–∞–Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞.")

# --- NEW: Function to initialize FinRobot Agent ---
@st.cache_resource # Cache the agent resource itself
def initialize_finrobot_agent():
    """Initializes and returns a FinRobot agent configured with OAI_CONFIG_LIST."""
    try:
        llm_config = {
            "config_list": autogen.config_list_from_json(
                "OAI_CONFIG_LIST", # Assumes file is in the root directory
                filter_dict={"model": ["gpt-4-0125-preview"]}, # Or whichever model you have in the list
            ),
            "timeout": 120,
            "temperature": 0.2, # Lower temperature for more factual answers
        }
        # Create a basic assistant agent
        # You might want to customize the system message later
        assistant_agent = SingleAssistant(
            name="Portfolio_Analyst_Assistant",
            llm_config=llm_config,
            system_message="You are a helpful AI assistant specialized in analyzing portfolio performance data. Answer the user's questions based on the provided portfolio summary. Be concise and clear.",
            human_input_mode="NEVER", # Agent runs without asking for human input during its process
        )
        return assistant_agent
    except FileNotFoundError:
        st.error("Error: OAI_CONFIG_LIST file not found. Please ensure it exists in the project root.")
        return None
    except Exception as e:
        st.error(f"Error initializing FinRobot agent: {e}")
        traceback.print_exc() # Print detailed traceback to console/log
        return None
# --- End Function to initialize FinRobot Agent ---

# --- NEW: Function to format analysis results for LLM ---
def format_portfolio_data_for_llm(analysis_results):
    """Formats the portfolio analysis results into a string for the LLM agent."""
    if not analysis_results:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

    summary_parts = []

    # Extract metrics
    metrics_df = analysis_results.get('metrics')
    if metrics_df is not None and not metrics_df.empty:
        summary_parts.append("**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º:**")
        # Convert metrics DataFrame to string format
        # We can iterate through columns (strategies) or rows (metrics)
        # Let's format by strategy for clarity
        for strategy in metrics_df.columns:
            summary_parts.append(f"\n*–°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}*" )
            for metric, value in metrics_df[strategy].items():
                # Format based on metric type if possible (e.g., percentages)
                if isinstance(value, (int, float)):
                    if any(p in metric.lower() for p in ['%', 'cagr', 'drawdown', 'volatility', 'return']):
                        formatted_value = f"{value:.2%}"
                    elif any(r in metric.lower() for r in ['ratio']):
                         formatted_value = f"{value:.2f}"
                    else:
                         formatted_value = f"{value:,.2f}" # General float formatting
                else:
                     formatted_value = str(value)
                summary_parts.append(f"  - {metric}: {formatted_value}")
        summary_parts.append("\n") # Add spacing
    else:
        summary_parts.append("–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

    # Extract date range and final values from daily values DataFrame
    daily_values_df = analysis_results.get('portfolio_daily_values')
    if daily_values_df is not None and not daily_values_df.empty:
        start_date = daily_values_df.index.min().strftime('%Y-%m-%d')
        end_date = daily_values_df.index.max().strftime('%Y-%m-%d')
        summary_parts.append(f"**–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:** {start_date} - {end_date}\n")

        summary_parts.append("**–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º:**")
        final_values = daily_values_df.iloc[-1]
        for strategy, value in final_values.items():
             summary_parts.append(f"  - {strategy}: ${value:,.2f}")
        summary_parts.append("\n")
    else:
        summary_parts.append("–î–∞–Ω–Ω—ã–µ –æ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

    return "\n".join(summary_parts)
# --- End Function to format analysis results for LLM ---

'''
poetry run streamlit run auth_app.py
'''